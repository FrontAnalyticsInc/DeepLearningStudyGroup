{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=True)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Brown Corpus is a good dataset to begin training with\n",
    "The corpus is split into genres or categories shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adventure': 69342,\n",
       " 'belles_lettres': 173096,\n",
       " 'editorial': 61604,\n",
       " 'fiction': 68488,\n",
       " 'government': 70117,\n",
       " 'hobbies': 82345,\n",
       " 'humor': 21695,\n",
       " 'learned': 181888,\n",
       " 'lore': 110299,\n",
       " 'mystery': 57169,\n",
       " 'news': 100554,\n",
       " 'religion': 39399,\n",
       " 'reviews': 40704,\n",
       " 'romance': 70022,\n",
       " 'science_fiction': 14470}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of words per section in the Brown Corpus\n",
    "dict(zip(categories, [len(brown.words(categories=cat)) for cat in categories]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a classifier to tell us whether a text is Adventure or Romance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['adventure', 'romance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "adventure_paras = brown.paras(categories='adventure')\n",
    "adventure_tokens = brown.words(categories='adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "romance_paras = brown.paras(categories='romance')\n",
    "romance_tokens = brown.words(categories='romance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start extracting features from our data, let's split the data into training, test, and dev sets so that we can evaluate our neural network once we have finished. \n",
    "\n",
    "Let's do a 75 / 15 / 10 split between training, test, and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset, train=0.80, test=0.20, dev=None):\n",
    "    train_end = round(len(dataset) * train)\n",
    "    if dev:\n",
    "        test_end = train_end + round(len(dataset) * test)\n",
    "        return dataset[:train_end], dataset[train_end:test_end], dataset[test_end:]\n",
    "    else:\n",
    "        return dataset[:train_end], dataset[train_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adventure_train, adventure_test, adventure_dev = split(adventure_paras,\n",
    "                                                       train=0.75,\n",
    "                                                       test=0.15,\n",
    "                                                       dev=0.10)\n",
    "\n",
    "romance_train, romance_test, romance_dev = split(romance_paras,\n",
    "                                                       train=0.75,\n",
    "                                                       test=0.15,\n",
    "                                                       dev=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to start extracting features. We already have our tokens, let's start by creating a list of features that say whether or not we saw a given word in the text.\n",
    "\n",
    "While we do that, think about what other types of features you think would be useful in differentiating between adventure and romance texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n",
      "940\n"
     ]
    }
   ],
   "source": [
    "# count of paragraphs in the adventure training set\n",
    "print(len(adventure_train))\n",
    "print(len(romance_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step in creating a list of features that indicate if we saw a given word is to find out what the vocabulary is. The vocabulary is the set of all word types we encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(romance_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1387"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adventure_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13469"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we need to add in a zero count for all vocab then add counts for each observed\n",
    "vocab = set(adventure_tokens+romance_tokens)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(paragraph, vocab):\n",
    "    # take one training example and return a sparse feature vector\n",
    "    # init feature vec with zeros\n",
    "    feature_vec = np.zeros(len(vocab), dtype=int)\n",
    "    feature_dict = dict(zip(vocab, feature_vec))\n",
    "    for sentence in paragraph: \n",
    "        for token in sentence:\n",
    "            # ensure that feature is already in vocab\n",
    "            if token in feature_dict.keys():\n",
    "                feature_dict[token] = 1  # one hot encoding\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_feature_dict(adventure_paras[0], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "D_train_adv = [get_feature_dict(paragraph, vocab) for paragraph in adventure_train]\n",
    "D_train_rom = [get_feature_dict(paragraph, vocab) for paragraph in romance_train]\n",
    "\n",
    "# test data\n",
    "D_test_adv = [get_feature_dict(paragraph, vocab) for paragraph in adventure_test]\n",
    "D_test_rom = [get_feature_dict(paragraph, vocab) for paragraph in romance_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is D?\n",
    "\n",
    "D is a convention of Scikit Learn that represents an array of dictionaries. These dictionaries can be plugged into SKlearn's DictVectorizer to give us our numpy arrays that we will train and test our neural network on later.\n",
    "\n",
    "Since we have two classes, romance and adventure, we have two dictionaries in our collection D.\n",
    "\n",
    "Once we have added all of our features and their counts for each set of examples we will fit them into our feature vector using: X = v.fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_train_adv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have a dictionary for each class that shows which word types appear in that text. We have repeated this for each test set (train, test, dev). This set of features will probably be useful for our model, but a more useful set of features might be the bigrams or trigrams that appear. Let's add those to the dictionaries, D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = v.fit_transform(D_train_adv+D_train_rom)\n",
    "# X_train_rom = v.fit_transform(D_train_rom)\n",
    "\n",
    "X_test = v.fit_transform(D_test_adv+D_test_rom)\n",
    "# X_test_rom = v.fit_transform(D_test_rom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1980x13469 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 78282 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<396x13469 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14347 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1040"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D_train_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(D_class1, D_class2):\n",
    "    # return list of gold labels\n",
    "    y1 = [0] * len(D_class1)\n",
    "    y2 = [1] * len(D_class2)\n",
    "    y = y1+y2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = get_y(D_train_adv, D_train_rom)\n",
    "y_test = get_y(D_test_adv, D_test_rom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Perceptron().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_one(classifier, X_train, y_train, X_test, y_test): \n",
    "    # fitting\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # accuracy\n",
    "    results = {}\n",
    "    y_hat_train = classifier.predict(X_train)\n",
    "    accur_train = sum(y_hat_train == y_train) / len(y_train)  # train accuracy\n",
    "    y_hat_test = classifier.predict(X_test)\n",
    "    accur_test = sum(y_hat_test == y_test) / len(y_test)  # test accuracy\n",
    "    results['accur_train'] = accur_train\n",
    "    results['accur_test'] = accur_test\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accur_test': 0.55303030303030298, 'accur_train': 0.97272727272727277}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_evaluate_one(Perceptron(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers to be fitted and evaluated\n",
    "classifiers = [MultinomialNB(), \n",
    "               LogisticRegression(), \n",
    "               # svm.SVC(kernel='rbf'), \n",
    "               Perceptron(), \n",
    "               tree.DecisionTreeClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_all(classifiers, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for c in classifiers:\n",
    "        cname = str(c).split('(')[0]\n",
    "        print('Training '+cname+ '...')\n",
    "        results[cname] = fit_and_evaluate_one(c, X_train, y_train, X_test, y_test)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MultinomialNB...\n",
      "Training LogisticRegression...\n",
      "Training Perceptron...\n",
      "Training DecisionTreeClassifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DecisionTreeClassifier': {'accur_test': 0.54292929292929293,\n",
       "  'accur_train': 1.0},\n",
       " 'LogisticRegression': {'accur_test': 0.56060606060606055,\n",
       "  'accur_train': 0.99494949494949492},\n",
       " 'MultinomialNB': {'accur_test': 0.61111111111111116,\n",
       "  'accur_train': 0.95303030303030301},\n",
       " 'Perceptron': {'accur_test': 0.55303030303030298,\n",
       "  'accur_train': 0.97272727272727277}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_evaluate_all(classifiers, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MultinomialNB...\n",
      "Training LogisticRegression...\n",
      "Training Perceptron...\n",
      "Training DecisionTreeClassifier...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accur_test</th>\n",
       "      <th>accur_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.953030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.994949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.553030</td>\n",
       "      <td>0.972727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.527778</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        accur_test  accur_train\n",
       "MultinomialNB             0.611111     0.953030\n",
       "LogisticRegression        0.560606     0.994949\n",
       "Perceptron                0.553030     0.972727\n",
       "DecisionTreeClassifier    0.527778     1.000000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict = fit_and_evaluate_all(classifiers, X_train, y_train, X_test, y_test)\n",
    "result_df = pd.DataFrame.from_dict(result_dict).transpose().sort_values('accur_test', ascending=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this sparse parameter do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What other applications could this type of model work for? Classifying fake news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_general",
   "language": "python",
   "name": "data_science_general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
