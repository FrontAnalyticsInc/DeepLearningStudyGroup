{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=True)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Brown Corpus is a good dataset to begin training with\n",
    "The corpus is split into genres or categories shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adventure': 69342,\n",
       " 'belles_lettres': 173096,\n",
       " 'editorial': 61604,\n",
       " 'fiction': 68488,\n",
       " 'government': 70117,\n",
       " 'hobbies': 82345,\n",
       " 'humor': 21695,\n",
       " 'learned': 181888,\n",
       " 'lore': 110299,\n",
       " 'mystery': 57169,\n",
       " 'news': 100554,\n",
       " 'religion': 39399,\n",
       " 'reviews': 40704,\n",
       " 'romance': 70022,\n",
       " 'science_fiction': 14470}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of words per section in the Brown Corpus\n",
    "dict(zip(categories, [len(brown.words(categories=cat)) for cat in categories]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a classifier to tell us whether a text is News or Lore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['news', 'lore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_paras = brown.paras(categories='news')\n",
    "news_tokens = brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lore_paras = brown.paras(categories='lore')\n",
    "lore_tokens = brown.words(categories='lore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start extracting features from our data, let's split the data into training, test, and dev sets so that we can evaluate our neural network once we have finished. \n",
    "\n",
    "Let's do a 75 / 15 / 10 split between training, test, and dev sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset, train=0.80, test=0.20, dev=None):\n",
    "    train_end = round(len(dataset) * train)\n",
    "    if dev:\n",
    "        test_end = train_end + round(len(dataset) * test)\n",
    "        return dataset[:train_end], dataset[train_end:test_end], dataset[test_end:]\n",
    "    else:\n",
    "        return dataset[:train_end], dataset[train_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train, news_test, news_dev = split(news_paras,\n",
    "                                                       train=0.75,\n",
    "                                                       test=0.15,\n",
    "                                                       dev=0.10)\n",
    "\n",
    "lore_train, lore_test, lore_dev = split(lore_paras,\n",
    "                                                       train=0.75,\n",
    "                                                       test=0.15,\n",
    "                                                       dev=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to start extracting features. We already have our tokens, let's start by creating a list of features that say whether or not we saw a given word in the text.\n",
    "\n",
    "While we do that, think about what other types of features you think would be useful in differentiating between news and lore texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676\n",
      "902\n"
     ]
    }
   ],
   "source": [
    "# count of paragraphs in the news training set\n",
    "print(len(news_train))\n",
    "print(len(lore_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step in creating a list of features that indicate if we saw a given word is to find out what the vocabulary is. The vocabulary is the set of all word types we encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2234"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lore_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22522"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we need to add in a zero count for all vocab then add counts for each observed\n",
    "vocab = set(news_tokens+lore_tokens)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(paragraph, vocab):\n",
    "    # take one training example and return a sparse feature vector\n",
    "    # init feature vec with zeros\n",
    "    feature_vec = np.zeros(len(vocab), dtype=int)\n",
    "    feature_dict = dict(zip(vocab, feature_vec))\n",
    "    for sentence in paragraph: \n",
    "        for token in sentence:\n",
    "            # ensure that feature is already in vocab\n",
    "            if token in feature_dict.keys():\n",
    "                feature_dict[token] = 1  # one hot encoding\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_feature_dict(lore_paras[0], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "D_train_news = [get_feature_dict(paragraph, vocab) for paragraph in news_train]\n",
    "D_train_lore = [get_feature_dict(paragraph, vocab) for paragraph in lore_train]\n",
    "\n",
    "# test data\n",
    "D_test_news = [get_feature_dict(paragraph, vocab) for paragraph in news_test]\n",
    "D_test_lore = [get_feature_dict(paragraph, vocab) for paragraph in lore_test]\n",
    "\n",
    "# test data\n",
    "D_dev_news = [get_feature_dict(paragraph, vocab) for paragraph in news_dev]\n",
    "D_dev_lore = [get_feature_dict(paragraph, vocab) for paragraph in lore_dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is D?\n",
    "\n",
    "D is a convention of Scikit Learn that represents an array of dictionaries. These dictionaries can be plugged into SKlearn's DictVectorizer to give us our numpy arrays that we will train and test our neural network on later.\n",
    "\n",
    "Since we have two classes, news and lore, we have two dictionaries in our collection D.\n",
    "\n",
    "Once we have added all of our features and their counts for each set of examples we will fit them into our feature vector using: X = v.fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_train_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have a dictionary for each class that shows which word types appear in that text. We have repeated this for each test set (train, test, dev). This set of features will probably be useful for our model, but a more useful set of features might be the bigrams or trigrams that appear. Let's add those to the dictionaries, D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = v.fit_transform(D_train_news+D_train_lore)\n",
    "X_test = v.fit_transform(D_test_news+D_test_lore)\n",
    "X_dev = v.fit_transform(D_dev_news+D_dev_lore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2578x22522 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 106941 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<515x22522 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25841 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(D_class1, D_class2):\n",
    "    # return list of gold labels\n",
    "    y1 = [0] * len(D_class1)\n",
    "    y2 = [1] * len(D_class2)\n",
    "    y = y1+y2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = get_y(D_train_news, D_train_lore)\n",
    "y_test = get_y(D_test_news, D_test_lore)\n",
    "y_dev = get_y(D_dev_news, D_dev_lore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Perceptron().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_one(classifier, X_train, y_train, X_test, y_test): \n",
    "    # fitting\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # accuracy\n",
    "    results = {}\n",
    "    y_hat_train = classifier.predict(X_train)\n",
    "    accur_train = sum(y_hat_train == y_train) / len(y_train)  # train accuracy\n",
    "    y_hat_test = classifier.predict(X_test)\n",
    "    accur_test = sum(y_hat_test == y_test) / len(y_test)  # test accuracy\n",
    "    results['accur_train'] = accur_train\n",
    "    results['accur_test'] = accur_test\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accur_test': 0.67766990291262141, 'accur_train': 0.99650892164468585}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_evaluate_one(Perceptron(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers to be fitted and evaluated\n",
    "classifiers = [MultinomialNB(), \n",
    "               LogisticRegression(), \n",
    "               # svm.SVC(kernel='rbf'), \n",
    "               Perceptron(), \n",
    "               tree.DecisionTreeClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_all(classifiers, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for c in classifiers:\n",
    "        cname = str(c).split('(')[0]\n",
    "        print('Training '+cname+ '...')\n",
    "        results[cname] = fit_and_evaluate_one(c, X_train, y_train, X_test, y_test)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MultinomialNB...\n",
      "Training LogisticRegression...\n",
      "Training Perceptron...\n",
      "Training DecisionTreeClassifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DecisionTreeClassifier': {'accur_test': 0.68737864077669908,\n",
       "  'accur_train': 1.0},\n",
       " 'LogisticRegression': {'accur_test': 0.72233009708737861,\n",
       "  'accur_train': 0.99844840961986037},\n",
       " 'MultinomialNB': {'accur_test': 0.72427184466019412,\n",
       "  'accur_train': 0.98564778898370831},\n",
       " 'Perceptron': {'accur_test': 0.67766990291262141,\n",
       "  'accur_train': 0.99650892164468585}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_and_evaluate_all(classifiers, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MultinomialNB...\n",
      "Training LogisticRegression...\n",
      "Training Perceptron...\n",
      "Training DecisionTreeClassifier...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accur_test</th>\n",
       "      <th>accur_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.724272</td>\n",
       "      <td>0.985648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.722330</td>\n",
       "      <td>0.998448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.704854</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.677670</td>\n",
       "      <td>0.996509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        accur_test  accur_train\n",
       "MultinomialNB             0.724272     0.985648\n",
       "LogisticRegression        0.722330     0.998448\n",
       "DecisionTreeClassifier    0.704854     1.000000\n",
       "Perceptron                0.677670     0.996509"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict = fit_and_evaluate_all(classifiers, X_train, y_train, X_test, y_test)\n",
    "result_df = pd.DataFrame.from_dict(result_dict).transpose().sort_values('accur_test', ascending=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What other applications could this type of model work for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How could we approach a task like classifying fake news?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have run a few quick classifiers at this dataset, how do you think the neural network will compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_general",
   "language": "python",
   "name": "data_science_general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
